{"meta":{"title":"逍客 - Stay Happy and Enjoy Life!","subtitle":"保持快乐 享受生活; 放下自我 换位思维。","description":"旧书的个人网站，记录生活点滴，也可能记录了各种心得；同时会分享一些没好的照片和户外活动内容。","author":"Joe Li","url":"https://choelea.github.io"},"pages":[],"posts":[{"title":"Hello World","slug":"hello-world","date":"2018-12-11T09:52:56.707Z","updated":"2018-12-11T09:52:56.708Z","comments":true,"path":"hello-world/","link":"","permalink":"https://choelea.github.io/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Spring Data Elasticsearch 快速上手全文检索","slug":"Elastic-Technologies/spring-data-elasticsearch-quick-start","date":"2018-12-11T09:52:56.707Z","updated":"2018-12-11T09:52:56.707Z","comments":true,"path":"Elastic-Technologies/spring-data-elasticsearch-quick-start/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/spring-data-elasticsearch-quick-start/","excerpt":"","text":"通过Spring Data Elasticsearch 实现全文检索; 通过指定 boost 来控制查询语句的相对的权重; 通过自定义ResultMapper 来实现查询聚合功能。 Elasticsearch 基础知识建立对Elasticsearch的初步的认识可以参考：https://mp.weixin.qq.com/s/stC_xMP1n3aQ-0ZNAc3eQA 上面的有些解释只是为了方便初学者快速掌握知识。ES的索引的Type在后期ES中会逐渐消失。https://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html 官方的中文文档参考： https://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html 版本 Spring Boot： 1.4.7 Spring Data Elasticsearch 2.0.11 Elasticsearch server 2.4 追加补充： 在随后的版本spring-data-elasticsearch 2.1.10.RELEASE 中增加了AggregatedPage ， 同时org.springframework.data.elasticsearch.core.DefaultResultMapper 也增加了聚合的支持。 笔者这里针对聚合的有部分工作，在2.1.10.RELEASE种已经不在需要，或者可以更优化一些。 安装与运行Elasticsearch 安装及运行elasticsearch 是运行于java之上，可以直接下载运行。从https://www.elastic.co/downloads/past-releases/elasticsearch-2-4-6 下载2.4.6 版本。 ZIP sha —– window安装包 TAR sha —– Mac 或者linux安装包 DEB sha RPM sha —– linux rpm 包可以安装成系统服务 ZIP包解压的直接进入 bin 目录运行 ./elasticserach, 运行 ./elasticsearch -d 后台运行 RPM 安装后通过 service elasticsearch start 来启动 Kibana 安装及运行 和es的类似，elasticsearch2.4 对应kibana的版本是4.6； 下载地址： https://www.elastic.co/downloads/past-releases/kibana-4-6-6 kibana 解压方式无后台运行命令，建议通过rpm方式安装sudo rpm -ivh kibana-4.6.6-x86_64.rpm , 以服务方式启动和停止。 安装Kibana的Sense插件此版本下没有dev tools，需要单独安装sense插件, 进入Kibana 的安装目录 /opt/kibana 运行./bin/kibana plugin --install elastic/sense。 代码https://github.com/choelea/spring-data-elasticsearch-quick-start 2.0.11.RELEASE123git clone https://github.com/choelea/spring-data-elasticsearch-quick-startcd spring-data-elasticsearch-quick-start/git checkout tags/2.0.11.RELEASE 最新的master的代码升级Spring Boot到1.5.13.RELEASE， 对应的spring-data-elasticsearch 自动升级至2.1.12.RELEASE， 在此版本基础上，DefaultResultMapper 已经支持了聚合。无需为聚合儿自定义ResultMapper。 配置12spring.data.elasticsearch.repositories.enabled = truespring.data.elasticsearch.cluster-nodes : 192.168.1.99:9300 定义Document参考： com.joe.springdataelasticsearch.document.ProductDoc . 定义文档需要注意必须有个id字段或者通过注解指定一个id字段，只有在有ID得情况下，文档才可以被更新。 否则会抛出异常：No id property found for class com.joe.springdataelasticsearch.document.ProductDoc 当前版本下需要指定Field的type，否则也会报错。 修改FieldType 会导致无法通过程序启动异常，需要手动删除后创建索引。 比如: 原有的type字段的FieldType是Long，改成String后会出现类似如下错误：mapper [type] of different type, current_type [long], merged_type [string] 创建索引系统启动后，创建索引和创建/更新mapping 123elasticsearchTemplate.deleteIndex(ProductDoc.class);elasticsearchTemplate.createIndex(ProductDoc.class);elasticsearchTemplate.putMapping(ProductDoc.class); 启动后可以通过http://192.168.1.99:9200/product-index/_mapping/main/ 来查看mapping。 Notes：文档索引的mapping的创建，不会因为注解@Document， 而是因为ElasticsearchRepository的存在。删除ProductDocRespository，可以发现启动服务后，文档不会自动创建。 参考问题：https://stackoverflow.com/questions/29496081/spring-data-elasticsearchs-field-annotation-not-working 索引文档elasticsearch 是通过PUT接口来索引文档。https://www.elastic.co/guide/cn/elasticsearch/guide/current/index-doc.html。 在使用Spring Data Elasticsearch的的时候，我们可以很方便的通过防JPA Repository的方式来操作; ProductDocRespository.save(ProductDoc doc) 来索引和更新文档。1public interface ProductDocRespository extends ElasticsearchRepository&lt;ProductDoc, Long&gt; 参考com.joe.springdataelasticsearch.listner.ContextRefreshedListener 来查看索引测试文档数据。 测试数据 全文检索查询主要解决： 多个字段搜索查询使用布尔匹配的方式， 参考官方说明 布尔匹配 不同字段的权重设置，采用设置Boost方式， 参考： 查询语句提升权重 聚合结果集 具体代码参考如下：123456789101112131415161718public Page&lt;ProductDoc&gt; search(String keyword, Boolean isSelfRun, Pageable pageable) &#123; BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery(); if (StringUtils.isNotEmpty(keyword)) &#123; queryBuilder.should(QueryBuilders.matchQuery(ProductDoc._name, keyword).boost(3)); // 给name字段更高的权重 queryBuilder.should(QueryBuilders.matchQuery(ProductDoc._description, keyword)); // description 默认权重 1 queryBuilder.minimumNumberShouldMatch(1); // 至少一个should条件满足 &#125; if (isSelfRun!=null &amp;&amp; isSelfRun) &#123; queryBuilder.must(QueryBuilders.matchQuery(ProductDoc._isSelfRun, Boolean.TRUE)); // 精准值条件查询 &#125; SearchQuery searchQuery = new NativeSearchQueryBuilder().withQuery(queryBuilder) .withPageable(pageable).build(); LOGGER.info(&quot;\\n search(): searchContent [&quot; + keyword + &quot;] \\n DSL = \\n &quot; + searchQuery.getQuery().toString()); return productDocRespository.search(searchQuery); &#125; 测试全文检索 http://localhost:8080/products?keyword=huawei http://localhost:8080/products?keyword=iphone 通过对iphone的搜索可以验证boost值得效果 http://localhost:8080/products?keyword=iphone&amp;isSelfRun=true 验证精准值匹配效果 聚合查询需求： 统计搜索出来的智能手机和普通手机的数量，从而提供进一步的过滤。 聚合的详细理解参考 聚合 | Elasticsearch: 权威指南 | Elastic， 这里我们只通过简单的桶（Bucket）的方式来实现需求。 通过google搜索spring data elasticsearch aggregation example 不难找到类似如下链接中的代码：https://github.com/spring-projects/spring-data-elasticsearch/blob/master/src/test/java/org/springframework/data/elasticsearch/core/aggregation/ElasticsearchTemplateAggregationTests.java； 但是我们需要同时返回桶的信息和检索出来的分页信息。如何利用Spring Data Elasticsearch来完成？ 通过查看spring-data-elasticsearch的源代码我们可以发现org.springframework.data.elasticsearch.core.DefaultResultMapper 会被默认用来返回分页检索出来的数据。需要同时返回分页数据及桶的数据，我们就需要定制一个ResultMapper； 参考：com.joe.springdataelasticsearch.core.ProductDocAggregationResultMapper。 聚合查询测试http://localhost:8080/products/aggregation?keyword=China 可以查出总共有5条结果，聚合返回告诉你其中有4个智能手机，1个普通手机。返回JSON 数据如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&#123; &quot;content&quot;: [ &#123; &quot;id&quot;: 2, &quot;name&quot;: &quot;Huawei&quot;, &quot;description&quot;: &quot;Made by China&quot;, &quot;type&quot;: &quot;SMARTPHONE&quot;, &quot;isSelfRun&quot;: false &#125;, &#123; &quot;id&quot;: 5, &quot;name&quot;: &quot;Iphone X&quot;, &quot;description&quot;: &quot;Iphone X is made by China&quot;, &quot;type&quot;: &quot;SMARTPHONE&quot;, &quot;isSelfRun&quot;: true &#125;, &#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;Mac Pro&quot;, &quot;description&quot;: &quot;Mac Pro is made by China&quot;, &quot;type&quot;: &quot;SMARTPHONE&quot;, &quot;isSelfRun&quot;: true &#125;, &#123; &quot;id&quot;: 7, &quot;name&quot;: &quot;Nokia N90&quot;, &quot;description&quot;: &quot;Nokia N 90 is made by China&quot;, &quot;type&quot;: &quot;GENERAL&quot;, &quot;isSelfRun&quot;: false &#125;, &#123; &quot;id&quot;: 3, &quot;name&quot;: &quot;Huawei Max3&quot;, &quot;description&quot;: &quot;Huawei is designed / made by China&quot;, &quot;type&quot;: &quot;SMARTPHONE&quot;, &quot;isSelfRun&quot;: false &#125; ], &quot;bucketsByType&quot;: [ &#123; &quot;key&quot;: &quot;SMARTPHONE&quot;, &quot;lable&quot;: &quot;type&quot;, &quot;docCount&quot;: 4 &#125;, &#123; &quot;key&quot;: &quot;GENERAL&quot;, &quot;lable&quot;: &quot;type&quot;, &quot;docCount&quot;: 1 &#125; ], &quot;totalElements&quot;: 5, &quot;last&quot;: true, &quot;totalPages&quot;: 1, &quot;number&quot;: 0, &quot;size&quot;: 10, &quot;sort&quot;: null, &quot;numberOfElements&quot;: 5, &quot;first&quot;: true&#125; 高亮显示参考：http://tech.jiu-shu.com/Elastic-Technologies/spring-data-elasticsearch-highlight","categories":[],"tags":[]},{"title":"Spring Data Elasticsearch 快速上手全文检索 - 进阶","slug":"Elastic-Technologies/spring-data-elasticsearch-quick-start-2","date":"2018-12-11T09:52:56.706Z","updated":"2018-12-11T09:52:56.706Z","comments":true,"path":"Elastic-Technologies/spring-data-elasticsearch-quick-start-2/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/spring-data-elasticsearch-quick-start-2/","excerpt":"","text":"继上一篇 Spring Data Elasticsearch 快速上手全文检索之后，进一步深入以下内容： 高亮显示关键词 指定Analyzer更合理的检索 最新的master的代码升级Spring Boot到1.5.13.RELEASE， 对应的spring-data-elasticsearch 自动升级至2.1.12.RELEASE， 在此版本基础上，DefaultResultMapper 已经支持了聚合。无需为聚合儿自定义ResultMapper。 代码1git clone https://github.com/choelea/spring-data-elasticsearch-quick-start https://github.com/elastic/elasticsearch/issues/11713 高亮关键词对name和description中的关键字进行高亮显示，直接参考代码：1234SearchQuery searchQuery = new NativeSearchQueryBuilder().withQuery(queryBuilder) .withPageable(pageable) .withHighlightFields( new HighlightBuilder.Field(ProductDoc._name).forceSource(true), new HighlightBuilder.Field(ProductDoc._description).forceSource(true)) .addAggregation(termBuilder).build(); 默认情况下返回高亮字段不在_source内，当转成成我们的ProductDoc的时候对应的name和description是不会有变化的， 这个时候还是需要定制ResultMapper， 因此这里定制了一个ExtResultMapper。 将高亮字段覆盖到ProductDoc 中对应的字段去。","categories":[],"tags":[]},{"title":"文档的title，页面及列表都会展示","slug":"Elastic-Technologies/spring-data-elasticsearch-2.1.12-highlight","date":"2018-12-11T09:52:56.703Z","updated":"2018-12-11T09:52:56.703Z","comments":true,"path":"Elastic-Technologies/spring-data-elasticsearch-2.1.12-highlight/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/spring-data-elasticsearch-2.1.12-highlight/","excerpt":"","text":"这个是一个模板, 请务必将showOnHome 修改为true 欢迎使用本Markdown编辑器使用simplemde-plus，用它写博客，将会带来全新的体验哦： Markdown和扩展Markdown简洁的语法 代码块高亮 图片链接和图片上传 丰富的快捷键 快捷键 Cmd-‘ 引用 Cmd-B 加粗 Cmd-E 清除Block Cmd-H 标题Header变小 Cmd-I 斜体 Cmd-K 链接 Cmd-L 无序列表 Cmd-P Preview Cmd-Alt-C 代码块 Cmd-Alt-I 插入图片 Cmd-Alt-L 有序列表 Shift-Cmd-H 标题Header变大 F9 窗口拆分 F11 全屏","categories":[],"tags":[]},{"title":"","slug":"Elastic-Technologies/elasticsearch-fuzzy-query","date":"2018-12-11T09:52:56.698Z","updated":"2018-12-11T09:52:56.698Z","comments":true,"path":"Elastic-Technologies/elasticsearch-fuzzy-query/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/elasticsearch-fuzzy-query/","excerpt":"","text":"title: Elasticsearch 模糊匹配description: fuzzy 查询是一个词项级别的查询，所以它不做任何分析。它通过某个词项以及指定的 fuzziness 查找到词典中所有的词项。 fuzziness 默认设置为 AUTO 。… 这个是一个模板, 请务必将showOnHome 修改为true https://www.elastic.co/guide/cn/elasticsearch/guide/cn/fuzzy-scoring.html以下来自123456模糊性评分编辑用户喜欢模糊查询。他们认为这种查询会魔法般的找到正确拼写组合。 很遗憾，实际效果平平。假设我们有1000个文档包含 ``Schwarzenegger`` ，只是一个文档的出现拼写错误 ``Schwarzeneger`` 。 根据 term frequency/inverse document frequency 理论，这个拼写错误文档比拼写正确的相关度更高，因为错误拼写出现在更少的文档中！换句话说，如果我们对待模糊匹配 类似其他匹配方法，我们将偏爱错误的拼写超过了正确的拼写，这会让用户抓狂。","categories":[],"tags":[]},{"title":"kibana的访问控制 - Nginx 反向代理 - 免费","slug":"Elastic-Technologies/Nignx-Kibana-Security","date":"2018-12-11T09:52:56.694Z","updated":"2018-12-11T09:52:56.694Z","comments":true,"path":"Elastic-Technologies/Nignx-Kibana-Security/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Nignx-Kibana-Security/","excerpt":"","text":"前一篇 Kibana 5.x 加强安全 采用的是官方的x-pack 插件来实现elastic技术栈的相关产品的权限控制。功能不错，也提供了很大的灵活性，不过x-pack并非免费产品；咨询了下licence价格，大概三个节点年费六千多美刀。。。废话不多说了，想想替代方案 - Nginx 反向代理 （收回5601端口，通过nginx反向代理+basic authentication来保证安全） 参考：How To Create a Self-Signed SSL Certificate for Nginx on CentOS 7 配置Nginx SSL第一步: 安装 Nginx 并配置防火墙参考上面的文章 注意： 80 和 443 端口必须对外打开。 当遇见ERR_CONNECTION_REFUSED 这类错误的时候，一定要提高警惕查看端口是否打开。以免浪费时间在配置上面。可以ssh到nginx机器上通过curl 的命令来验证，如果服务器上curl可以访问，外面不可访问；那么很可能端口没开放 第二步：生成证书参考上面的文章 第三步：添加kibana.https.conf配置配置如下： 1234567891011121314151617181920212223242526272829303132333435363738394041server &#123; listen 443 http2 ssl; listen [::]:443 http2 ssl; server_name kibana.domain.com; ssl_certificate /etc/ssl/certs/nginx-selfsigned.crt; ssl_certificate_key /etc/ssl/private/nginx-selfsigned.key; ssl_dhparam /etc/ssl/certs/dhparam.pem; ######################################################################## # from https://cipherli.st/ # # and https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html # ######################################################################## ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers &quot;EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH&quot;; ssl_ecdh_curve secp384r1; ssl_session_cache shared:SSL:10m; ssl_session_tickets off; ssl_stapling on; ssl_stapling_verify on; resolver 8.8.8.8 8.8.4.4 valid=300s; resolver_timeout 5s; # Disable preloading HSTS for now. You can use the commented out header line that includes # the &quot;preload&quot; directive if you understand the implications. #add_header Strict-Transport-Security &quot;max-age=63072000; includeSubdomains; preload&quot;; add_header Strict-Transport-Security &quot;max-age=63072000; includeSubdomains&quot;; add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; ################################## # END https://cipherli.st/ BLOCK # ################################## # 这里是反向代理到kibana服务 走http协议 location / &#123; proxy_pass http://localhost:5601; &#125;&#125; 第四步：验证SSL 访问为设置http跳转的时候，注意在浏览器地址栏中输入https://kibana.domain.com 来验证 第五步： 添加Nginx的Basic Authentication 访问控制 查看是否有安装httpd-tools sudo rpm -qa | grep httpd-tools, 如果有，则可以看到如下信息：httpd-tools-2.4.6-40.el7.centos.4.x86_64 如果没有安装，可以通过sudo yum -y install httpd-tools 来安装 配置nginx 反向代理 添加 12auth_basic &quot; Basic Authentication &quot;; auth_basic_user_file &quot;/etc/nginx/.htpasswd&quot;; 添加至反向代理的配置 1234567......location / &#123; proxy_pass http://localhost:5601; auth_basic &quot; Basic Authentication &quot;; auth_basic_user_file &quot;/etc/nginx/.htpasswd&quot;; &#125;..... 生成密码文件 sudo htpasswd -c /etc/nginx/.htpasswd username 根据提示输入密码 重新加载ngixn sudo service nginx reload 再次登录来，提示弹出框，输入用户名和密码","categories":[],"tags":[]},{"title":"Metricbeat 的使用","slug":"Elastic-Technologies/Metricbeat-Usage","date":"2018-12-11T09:52:56.693Z","updated":"2018-12-11T09:52:56.693Z","comments":true,"path":"Elastic-Technologies/Metricbeat-Usage/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Metricbeat-Usage/","excerpt":"","text":"目标统计并展示系统的信息 cpu， 内存等 (当然metricbeat能收集的信息种类还很多) 前提 版本： 5.x 已经安装了ELK (elasticsearch, logstash (可选）, kibana) 安装了x-pack （配置了对应的security）（可选） 参考 Kibana 5.x 加强安全 安装配置安装，配置参考 官方网站使用OOTB配置即可，一般只需要修改ES的端口和地址。 如果加强了security，也需要更改 metricbeat.yml。 这里已经加强了安全，配置了用户，故需要更改metricbeat.yml添加elasticsearch的相关访问用户。（创建角色和用户可以参考 Kibana 5.x 加强安全 ，这里角色需要用操作索引metricbeat-*） elasticsearch 默认绑定了localhost的访问，需要取消这种绑定。 设置network.host: 0.0.0.0 0.0.0.0 表示任意地址，如果设置成了IP地址，那么同台机器的kibana和logstash的需要做对应的修改。（比如：192.168.1.50， logstash和kibana需要把链接elasticsearch的hosts 从localhost改成：192.168.1.50） 加载kibana的示例 index template 和 dashboards 因为metricbeat 可能装在多个机器，index template 和dashboard 只需要导入一次即可。默认会自动加载index template到elasticsearch。 1./scripts/import_dashboards -es http://localhost:9200 -user elastic -pass changeme kibana中查看对应的结果登录kibana打开对应的dashboard 既可以看到统计报告了","categories":[],"tags":[]},{"title":"Logstash Filter 配置","slug":"Elastic-Technologies/Logstash-Filter","date":"2018-12-11T09:52:56.692Z","updated":"2018-12-11T09:52:56.692Z","comments":true,"path":"Elastic-Technologies/Logstash-Filter/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Logstash-Filter/","excerpt":"","text":"笔者这里仅仅列出配置文件，在研究之后最红并没有采用在logstash的接下日志为json的做法。而是将json的输出放在了各个服务/应用中处理， spring boot的app可以参考：logstash-logback-encoder1234567891011121314151617181920212223242526272829303132333435input &#123; beats &#123; port =&gt; 5044 &#125;&#125;filter &#123; #If log line contains tab character followed by &apos;at&apos; then we will tag that entry as stacktrace if [message] =~ &quot;\\tat&quot; &#123; grok &#123; match =&gt; [&quot;message&quot;, &quot;^(\\tat)&quot;] add_tag =&gt; [&quot;stacktrace&quot;] &#125; &#125; #Grokking Spring Boot&apos;s default log format grok &#123; match =&gt; [ # Record transaction &quot;message&quot;,&quot;(?&lt;timestamp&gt;%&#123;YEAR&#125;-%&#123;MONTHNUM&#125;-%&#123;MONTHDAY&#125; %&#123;TIME&#125;) %&#123;LOGLEVEL:level&#125; %&#123;NUMBER:pid&#125; --- \\[\\s*(?&lt;thread&gt;[^\\]]+)\\] (?&lt;class&gt;[A-Za-z0-9.#_]+)\\s*: \\[\\s*(?&lt;transactionInfo&gt;[^\\]]+)\\]&quot;, &quot;message&quot;, &quot;(?&lt;timestamp&gt;%&#123;YEAR&#125;-%&#123;MONTHNUM&#125;-%&#123;MONTHDAY&#125; %&#123;TIME&#125;) %&#123;LOGLEVEL:level&#125; %&#123;NUMBER:pid&#125; --- \\[\\s*(?&lt;thread&gt;[^\\]]+)\\] (?&lt;class&gt;[A-Za-z0-9.#_]+)\\s*:\\s+(?&lt;logmessage&gt;.*)&quot;, &quot;message&quot;, &quot;(?&lt;timestamp&gt;%&#123;YEAR&#125;-%&#123;MONTHNUM&#125;-%&#123;MONTHDAY&#125; %&#123;TIME&#125;) %&#123;LOGLEVEL:level&#125; %&#123;NUMBER:pid&#125; --- .+? :\\s+(?&lt;logmessage&gt;.*)&quot; ] &#125; #Parsing out timestamps which are in timestamp field thanks to previous grok section date &#123; match =&gt; [ &quot;timestamp&quot; , &quot;yyyy-MM-dd HH:mm:ss.SSS&quot; ] &#125;&#125;output &#123; elasticsearch&#123;&#125; stdout&#123; codec =&gt; rubydebug &#125;&#125; 这里grok配置了三册过滤， 第一层用作统计，message的格式如下： 12016-07-15 20:30:30.884 INFO 14624 --- [nio-8081-exec-3] c.l.a.w.controller.OfbizProxyController : [&#123;&quot;transactionCode&quot;:&quot;ofbizProxy&quot;,&quot;transactionDuration&quot;:246&#125;] 使用Grok Debugger 解析后如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&#123; \"timestamp\": [ [ \"2016-07-15 20:30:30.884\" ] ], \"YEAR\": [ [ \"2016\" ] ], \"MONTHNUM\": [ [ \"07\" ] ], \"MONTHDAY\": [ [ \"15\" ] ], \"TIME\": [ [ \"20:30:30.884\" ] ], \"HOUR\": [ [ \"20\" ] ], \"MINUTE\": [ [ \"30\" ] ], \"SECOND\": [ [ \"30.884\" ] ], \"level\": [ [ \"INFO\" ] ], \"pid\": [ [ \"14624\" ] ], \"BASE10NUM\": [ [ \"14624\" ] ], \"thread\": [ [ \"nio-8081-exec-3\" ] ], \"class\": [ [ \"c.l.a.w.controller.OfbizProxyController\" ] ], \"transactionInfo\": [ [ \"&#123;\"transactionCode\":\"ofbizProxy\",\"transactionDuration\":246&#125;\" ] ]&#125; 第二层针对普通的log 12016-07-15 20:30:07.768 INFO 14624 --- [nio-8081-exec-1] c.l.a.web.controller.LoginController : Login username:vincent.chen@okchem.com IP is:0:0:0:0:0:0:0:1 解析后的json如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&#123; &quot;timestamp&quot;: [ [ &quot;2016-07-15 20:30:07.768&quot; ] ], &quot;YEAR&quot;: [ [ &quot;2016&quot; ] ], &quot;MONTHNUM&quot;: [ [ &quot;07&quot; ] ], &quot;MONTHDAY&quot;: [ [ &quot;15&quot; ] ], &quot;TIME&quot;: [ [ &quot;20:30:07.768&quot; ] ], &quot;HOUR&quot;: [ [ &quot;20&quot; ] ], &quot;MINUTE&quot;: [ [ &quot;30&quot; ] ], &quot;SECOND&quot;: [ [ &quot;07.768&quot; ] ], &quot;level&quot;: [ [ &quot;INFO&quot; ] ], &quot;pid&quot;: [ [ &quot;14624&quot; ] ], &quot;BASE10NUM&quot;: [ [ &quot;14624&quot; ] ], &quot;thread&quot;: [ [ &quot;nio-8081-exec-1&quot; ] ], &quot;class&quot;: [ [ &quot;c.l.a.web.controller.LoginController&quot; ] ], &quot;logmessage&quot;: [ [ &quot;Login username:vincent.chen@okchem.com IP is:0:0:0:0:0:0:0:1&quot; ] ]&#125; 第三层针对遗漏的无法匹配到的log再次解析， 这里暂时没有示例","categories":[],"tags":[]},{"title":"Elasticsearch 自定义Mapping","slug":"Elastic-Technologies/Elasticsearch-Mapping","date":"2018-12-11T09:52:56.691Z","updated":"2018-12-11T09:52:56.692Z","comments":true,"path":"Elastic-Technologies/Elasticsearch-Mapping/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Elasticsearch-Mapping/","excerpt":"","text":"Mapping 定义前面有一个篇简单的关于mapping的博客，当时是基于2.4 版本。 elastic技术栈在最近很活跃，目前版本已经更新至5.x。5.x有了比较大的变化。2.4 版本的定义在5.x上大部分已经失去了意义。（比如：5.x已经不再支持string 类型）这里截取一点官网对应的定义： elasticsearch 通过定义的映射mapping来决定文档及其字段改如何被存储和索引。比如：字段是否可以支持全文搜索; 字段是否包含日期，地理位置; 日期的格式; 自定义自动映射的规则。 基于5.x，前面博客 提到的user，uri等字段就可以使用keyword type。 12345678910PUT /business-index-*/_mapping/business&#123; &quot;properties&quot; : &#123; &quot;uri&quot; : &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;user&quot; : &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;keyword&quot; : &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;responseStatus&quot; : &#123; &quot;type&quot; : &quot;integer&quot; &#125;, &quot;responseTime&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125;&#125; elastic的文档维护的算是比较好的，基本英语OK的都是直接去参考官方文档。 mapping的更新可以参考 elastic 官网","categories":[],"tags":[]},{"title":"使用ELK来做日志归总","slug":"Elastic-Technologies/Elasticsearch-Logstash-Kibana-Log-Collecting","date":"2018-12-11T09:52:56.691Z","updated":"2018-12-11T09:52:56.691Z","comments":true,"path":"Elastic-Technologies/Elasticsearch-Logstash-Kibana-Log-Collecting/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Elasticsearch-Logstash-Kibana-Log-Collecting/","excerpt":"","text":"ELK 初探ELK实时日志分析平台 初次尝试。 ELK 的多种架构请参考文章: 漫谈ELK在大数据运维中的应用 平台 CentOS 7 Oracle JDK 8 Kibana 4.5.2 Elaticsearch 2.3.4 logstash 2.3.4 filebeat 1.2.3查看version command： filebeat --version系统架构图软件的安装采用yum的安装模式。首先需要添加对应的repo文件。 对应的详细的安装方法可以参考在线文档， 这里以logstash为例。logstash 安装 Download and install the public signing key 1rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch 添加Repo到目录/etc/yum.repos.d/， 比如：logstash.repo 123456[logstash-2.3]name=Logstash repository for 2.3.x packagesbaseurl=https://packages.elastic.co/logstash/2.3/centosgpgcheck=1gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearchenabled=1 安装 1yum install logstash 随系统自动启动1sudo chkconfig --add filebeat 其他软件的repositoriesfilebeat 123456[beats]name=Elastic Beats Repositorybaseurl=https://packages.elastic.co/beats/yum/el/$basearchenabled=1gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearchgpgcheck=1 elasticsearch 官方介绍 123456[elasticsearch-2.x]name=Elasticsearch repository for 2.x packagesbaseurl=https://packages.elastic.co/elasticsearch/2.x/centosgpgcheck=1gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearchenabled=1 kibana 在线文档 123456[kibana-4.5]name=Kibana repository for 4.5.x packagesbaseurl=http://packages.elastic.co/kibana/4.5/centosgpgcheck=1gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearchenabled=1 查看服务状态 1servie logstash status 查看服务文件路径 1rpm -ql logstash FileBeat 使用filebeat 安装后的配置文件存放于：/etc/filebeat/下修改配置文件filebeat.yml1， 修改文件的路径：比如：/home/osboxes/app.log12345filebeat: prospectors: - paths: - &quot;/home/osboxes/app.log&quot; 2， 修改输出， 默认是直接输出到Elasticsearch，我们修改输出到logstash只需要打开对应的注释即可，将elasticsearch相关注释掉， 打开logstash的注释。 123456output: logstash: hosts: [&quot;127.0.0.1:5044&quot;] # Optional load balance the events between the Logstash hosts #loadbalance: true filebeat.yml 已经配置了多个output选项，我们只需要打开注解。 这里可以做个小的测试。 修改配置后可运行命令验证：filebeat -configtest -e. filebeat只能配置一个output项，修改配置后需要重启1，找到Console output，打开注解 1234##Console output console: # Pretty print json event pretty: true 2， 停止filebeat服务 sudo service filebeat stop，手动启动filebeat来方便我们观察console输出sudo filebeat -e -c /etc/filebeat/filebeat.yml。(On windows: filebeat.exe -e -c filebeat.yml)3， 新开窗口输出信息至文件/var/log/app.log1echo &quot;2016-06-29 17:14:13.802 INFO 6244 --- [main] org.hibernate.Version : HHH000412: Hibernate Core &#123;4.3.11.Final&#125;&quot; &gt;&gt; app.log 4，切换至filebeat的启动窗口可以看到如下的输出。 123456789101112131415[osboxes@osboxes logstash]$ sudo filebeat -e -c /etc/filebeat/filebeat.yml&#123; &quot;@timestamp&quot;: &quot;2016-07-11T13:44:43.926Z&quot;, &quot;beat&quot;: &#123; &quot;hostname&quot;: &quot;osboxes&quot;, &quot;name&quot;: &quot;osboxes&quot; &#125;, &quot;count&quot;: 1, &quot;fields&quot;: null, &quot;input_type&quot;: &quot;log&quot;, &quot;message&quot;: &quot;2016-06-29 17:14:13.802 INFO 6244 --- [main] org.hibernate.Version : HHH000412: Hibernate Core &#123;4.3.11.Final&#125;&quot;, &quot;offset&quot;: 130, &quot;source&quot;: &quot;/home/osboxes/app.log&quot;, &quot;type&quot;: &quot;log&quot;&#125; LogStash 配置 上面的小测做完后，将filebeat的配置改回输出到logstash。 连通filebeat和logstash 1， 添加logstash.conf 文件在/etc/logstash/conf.d/logstash.conf 123456789input &#123; beats &#123; port =&gt; 5044 &#125;&#125;output &#123; stdout&#123;&#125;&#125; 修改后可以通过命令验证配置是否正确： 1sudo /opt/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.conf --configtest 2, 启动logstash采用命令启动方便从console观察输出。sudo /opt/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.conf如果采用service的启动方式，需要去/var/log/logstash/logstash.stdout 查看log3，启动filebeat 然后向文件app.log 写入log 1echo &quot;2016-06-29 17:14:13.802 INFO 6244 --- [main] org.hibernate.Version : HHH000412: Hibernate Core &#123;4.3.11.Final&#125;&quot; &gt;&gt; app.log 4，切换至logstash窗口， 可以观察到一下输出，证明filebeat已经可以成功输出到logstash 1234[osboxes@osboxes bin]$ sudo ./logstash -f /etc/logstash/conf.d/logstash.conf Settings: Default pipeline workers: 1Pipeline main started2016-07-12T05:57:46.877Z osboxes 2016-06-29 17:14:13.802 INFO 6244 --- [main] org.hibernate.Version : HHH000412: Hibernate Core &#123;4.3.11.Final&#125; 使用Grok Filter Plugin解析日志 （spring boot 的默认日志格式）1， 修改logstash.conf 添加filter，重启logstash 1234567891011121314151617181920212223242526272829303132input &#123; beats &#123; port =&gt; 5044 &#125;&#125;filter &#123; #If log line contains tab character followed by &apos;at&apos; then we will tag that entry as stacktrace if [message] =~ &quot;\\tat&quot; &#123; grok &#123; match =&gt; [&quot;message&quot;, &quot;^(\\tat)&quot;] add_tag =&gt; [&quot;stacktrace&quot;] &#125; &#125; #Grokking Spring Boot&apos;s default log format grok &#123; match =&gt; [ &quot;message&quot;, &quot;(?&lt;timestamp&gt;%&#123;YEAR&#125;-%&#123;MONTHNUM&#125;-%&#123;MONTHDAY&#125; %&#123;TIME&#125;) %&#123;LOGLEVEL:level&#125; %&#123;NUMBER:pid&#125; --- \\[(?&lt;thread&gt;[A-Za-z0-9-]+)\\] (?&lt;class&gt;[A-Za-z0-9.#_]+)\\s*:\\s+(?&lt;logmessage&gt;.*)&quot;, &quot;message&quot;, &quot;(?&lt;timestamp&gt;%&#123;YEAR&#125;-%&#123;MONTHNUM&#125;-%&#123;MONTHDAY&#125; %&#123;TIME&#125;) %&#123;LOGLEVEL:level&#125; %&#123;NUMBER:pid&#125; --- .+? :\\s+(?&lt;logmessage&gt;.*)&quot; ] &#125; #Parsing out timestamps which are in timestamp field thanks to previous grok section date &#123; match =&gt; [ &quot;timestamp&quot; , &quot;yyyy-MM-dd HH:mm:ss.SSS&quot; ] &#125;&#125;output &#123; stdout&#123; codec =&gt; rubydebug &#125; 2，写入log到文件app.log 1echo &quot;2016-06-29 17:14:09.477 INFO 6244 --- [main] faultConfiguringBeanFactoryPostProcessor : No bean named &apos;errorChannel&apos; has been explicitly defined. Therefore, a default PublishSubscribeChannel will be created.&quot; &gt;&gt; app.log 3， 切换logstash查看输出 12345678910111213141516171819202122232425&#123; &quot;message&quot; =&gt; &quot;2016-06-29 17:14:09.477 INFO 6244 --- [main] faultConfiguringBeanFactoryPostProcessor : No bean named &apos;errorChannel&apos; has been explicitly defined. Therefore, a default PublishSubscribeChannel will be created.&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; &quot;2016-06-29T16:14:09.477Z&quot;, &quot;count&quot; =&gt; 1, &quot;fields&quot; =&gt; nil, &quot;source&quot; =&gt; &quot;/home/osboxes/app.log&quot;, &quot;offset&quot; =&gt; 987, &quot;type&quot; =&gt; &quot;log&quot;, &quot;input_type&quot; =&gt; &quot;log&quot;, &quot;beat&quot; =&gt; &#123; &quot;hostname&quot; =&gt; &quot;osboxes&quot;, &quot;name&quot; =&gt; &quot;osboxes&quot; &#125;, &quot;host&quot; =&gt; &quot;osboxes&quot;, &quot;tags&quot; =&gt; [ [0] &quot;beats_input_codec_plain_applied&quot; ], &quot;timestamp&quot; =&gt; &quot;2016-06-29 17:14:09.477&quot;, &quot;level&quot; =&gt; &quot;INFO&quot;, &quot;pid&quot; =&gt; &quot;6244&quot;, &quot;thread&quot; =&gt; &quot;main&quot;, &quot;class&quot; =&gt; &quot;faultConfiguringBeanFactoryPostProcessor&quot;, &quot;logmessage&quot; =&gt; &quot;No bean named &apos;errorChannel&apos; has been explicitly defined. Therefore, a default PublishSubscribeChannel will be created.&quot;&#125; 至此，完成了初步的日志的解析，日志别解析至对应的fields中。 接下来将这些数据推送至Elasticsearch进行索引。 修改logstash配置，输出到elasticsearch修改配置文件的output。 1234output &#123; elasticsearch &#123; &#125;&#125; 用这样的结构，Logstash使用http协议连接到Elasticsearch。上面的例子假设Logstash和Elasticsearch运行在同一个机器上。您可以使用主机配置hosts =&gt; &quot;es-machine:9092指定远程Elasticsearch实例。 查看结果一次启动elasticsearch，kibana，logstash，filebeat。 （filebeat已启动的话，无需重启） 安装Sense进入/opt/kibana/ 运行：$sudo ./bin/kibana plugin --install elastic/senseYou should now be able to access Sense with a web browser on http://localhost:5601/app/sense spring boot 日志配置尽量采用统一的日志输出格式1, JPA 的sql输出 12#spring.jpa.show-sql = true #不推荐这种方式logging.level.org.hibernate.SQL=DEBUG 常见的部署方式由于logstash比较消耗系统资源， 采用filebeat 来采集数据， 然后推送到logstash。 简单的case可以将logstash elasticsearch kibana 放在一个虚拟机。 filebeat可以分别安装在各个对应的微服务上。 注意：当这些部署在不同的机器上的时候，需要打开对应的端口。 对应的配置也需要相对修改下。打开logstash的端口： 12$ sudo firewall-cmd --zone=public --add-port=5044/tcp --permanent$ sudo firewall-cmd --reload filebeat的配置修改 123logstash: # The Logstash hosts hosts: [&quot;192.168.1.186:5044&quot;] 修改hostName如果微服务部署在不同的虚拟机中， 可以通过修改hostname，然后在ES的index中通过hostname 来区分日志的来源 12$ hostnamectl status# hostnamectl set-hostname Your-New-Host-Name-Here 关于日志采集的策略（网上未提及此topic）配置logstash是件麻烦事情。 一下两种策略互相冲突1， 保证所有的log都index到ES这中策略方便用户查找问题， 因为所有的log都可以搜索到2， 严格过滤， 只提取我们需要的log信息这种很方便做统计， 但是其他很多log会被过滤掉， 用来找问题不方便。 服务器时间设置最好保证日志源的服务器时间和ELK的数据库服务器时间一直1# ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime","categories":[],"tags":[]},{"title":"Kibana 5.x 加强安全 - x-pack篇","slug":"Elastic-Technologies/Elasticsearch-Kibana-Security-xpack","date":"2018-12-11T09:52:56.689Z","updated":"2018-12-11T09:52:56.690Z","comments":true,"path":"Elastic-Technologies/Elasticsearch-Kibana-Security-xpack/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Elasticsearch-Kibana-Security-xpack/","excerpt":"","text":"此文之前，假定读者已经一次完成了Kibana和elasticsearch的安装。参考官方文档，安装后默认配置已经可以连通kibana和es。 系统： centos7 内容： 增加authentication &amp; enable ssl elastic 技术栈 的另外一个重要的角色是x-pack. ES安装xpack插件参考安装xpackRun bin/elasticsearch-plugin install from ES_HOME on each node in your cluster:1bin/elasticsearch-plugin install x-pack Kibana 安装xpack 插件参考安装xpack Install X-Pack into Kibana by running bin/kibana-plugin in your Kibana installation directory.1bin/kibana-plugin install x-pack 依次启动elasticsearch 和kibana修改用户elastic 和 kibana的密码X-Pack 文档：修改密码 X-Pack security provides a built-in elastic superuser you can use to start setting things up. The default password for the elastic user is changeme. 123curl -XPUT -u elastic &apos;localhost:9200/_xpack/security/user/elastic/_password&apos; -d &apos;&#123; &quot;password&quot; : &quot;elasticpassword&quot;&#125;&apos; 123curl -XPUT -u elastic &apos;localhost:9200/_xpack/security/user/kibana/_password&apos; -d &apos;&#123; &quot;password&quot; : &quot;kibanapassword&quot;&#125;&apos; CURL授权在访问需要授权的页面时，可通过-u选项提供用户名和密码进行授权。 通常的做法是在命令行只输入用户名，之后会提示输入密码，这样可以保证在查看历史记录时不会将密码泄露 Enable Kibana SSLUsing Kibana in a Production Environment配置上证书的路径即可：123# SSL for outgoing requests from the Kibana Server (PEM formatted)server.ssl.key: /path/to/your/server.keyserver.ssl.cert: /path/to/your/server.crt 修改了超级用户的密码，enable ssl后，就可以放心的去使用kibana的Dev Tools 或者chrome插件（sense）进行大部分API 的操作。 （在此之前需要ssh到服务器通过curl来操作以保证安全） 创建用户logstash_writer官方参考上面步骤完成后会发现logstash推送给es报错了。因为现在ES需要用户名和密码了。 这里我们需要创建一个用户拥有write, delete, and create_index的权限。 12[2016-12-23T20:42:19,350][WARN ][logstash.outputs.elasticsearch] Attempted to resurrect connection to dead ES instance, but got an error. &#123;:url=&gt;#&lt;URI::HTTP:0x17b5a1bd URL:http://localhost:9200&gt;, :error_type=&gt;LogStash::Outputs::ElasticSearch::HttpClient::Pool::BadResponseCodeError, :error=&gt;&quot;Got response code &apos;401&apos; contact Elasticsearch at URL &apos;http://localhost:9200/&apos;&quot;&#125;[2016-12-23T20:42:20,132][WARN ][logstash.shutdownwatcher ] &#123;&#125; 先创建一个role：logstash_writer 12345678910POST _xpack/security/role/logstash_writer&#123; &quot;cluster&quot;: [&quot;manage_index_templates&quot;, &quot;monitor&quot;], &quot;indices&quot;: [ &#123; &quot;names&quot;: [ &quot;logstash-*&quot;,&quot;business-index-*&quot;], &quot;privileges&quot;: [&quot;write&quot;,&quot;delete&quot;,&quot;create_index&quot;] &#125; ]&#125; 再创建一个用户：logstash_internal拥有Role：logstash_writer 123456POST /_xpack/security/user/logstash_internal&#123; &quot;password&quot; : &quot;changeme&quot;, &quot;roles&quot; : [ &quot;logstash_writer&quot;], &quot;full_name&quot; : &quot;Internal Logstash User&quot;&#125; 上面的操作也可以通过Kibana的Management UI来操作 配置logstash.conf 123456output &#123; elasticsearch &#123; ... user =&gt; logstash_internal password =&gt; changeme &#125; logstash, elasticsearch, kibana 如果在同一网络，而暴露出去的只有kibana的话，logstash和elasticsearch 之前是无需授权的。可以参考Enabling Anonymous Access 另外，logstash和elasticsearch之间如果需要授权，会不会有性能的影响？ 给Kibana用户加上index的读的权限Kibana安装xpack后默认就需要登录了。也可以用超级用户elastic登录登录后打开DevTools进行ES API的操作。 修改后停掉kibana服务。修改kibana的配置： Once you change the password, you need to specify it with the elasticsearch.password property in kibana.yml: 1elasticsearch.password: &quot;s0m3th1ngs3cr3t&quot; 坑 （Tricky Part） /etc/logstash/conf.d 下不要有多余的文件。比如logstash.conf.bak， 似乎logstash会读这个文件夹下的不止logstash.conf这个文件配置。logstash.conf.bak 会导致死循环一样的重启。elastic community","categories":[],"tags":[]},{"title":"Elastcisearch 6.2 Restful API","slug":"Elastic-Technologies/Elasticsearch-6_2-Restful-API","date":"2018-12-11T09:52:56.689Z","updated":"2018-12-11T09:52:56.689Z","comments":true,"path":"Elastic-Technologies/Elasticsearch-6_2-Restful-API/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Elasticsearch-6_2-Restful-API/","excerpt":"","text":"Elastcisearch详细的API请参考官方网站： https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html 这里只列举常用的方式。 索引API官方链接： https://www.elastic.co/guide/en/elasticsearch/reference/6.2/indices.html 创建索引快速创建1PUT /news 创建名为test的索引，没有创建任何对应的Type,以及Mapping12345&#123; &quot;acknowledged&quot;: true, &quot;shards_acknowledged&quot;: true, &quot;index&quot;: &quot;news&quot;&#125; 查看索引1GET /news 123456789101112131415161718&#123; &quot;news&quot;: &#123; &quot;aliases&quot;: &#123;&#125;, &quot;mappings&quot;: &#123;&#125;, &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1535677066065&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;-fZX17QdQjWE_AK79pO8lQ&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;6020499&quot; &#125;, &quot;provided_name&quot;: &quot;news&quot; &#125; &#125; &#125;&#125; 删除索引1curl -XDELETE &quot;http://192.168.1.99:9200/news&quot; // 删除索引 1234&#123; &quot;ok&quot;: true, &quot;acknowledged&quot;: true&#125; 设置类型并定义Mapping (推荐)1234567891011121314151617181920PUT /news/_mapping/_doc&#123; &quot;properties&quot;:&#123; &quot;title&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125;, &quot;content&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125;, &quot;postDate&quot;:&#123; &quot;type&quot;:&quot;date&quot; &#125;, &quot;categories&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;tags&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125;&#125; title和content是用于全文检索的，同时需要分词的 categories tags无需分词，这里的categories和tags都会存放多个值的数组。关于数组类型参考Array DataType elasticsearch 支持 Dynamical Mapping, 大多数情况下，这都不是一个推荐方式。","categories":[],"tags":[]},{"title":"Elastcisearch 2.4 Restful API","slug":"Elastic-Technologies/Elasticsearch-2_4-Restful-Api","date":"2018-12-11T09:52:56.688Z","updated":"2018-12-11T09:52:56.688Z","comments":true,"path":"Elastic-Technologies/Elasticsearch-2_4-Restful-Api/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Elasticsearch-2_4-Restful-Api/","excerpt":"","text":"Elastcisearch详细的API请参考官方网站： https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html 这里只列举常用的方式。 索引API官方链接： https://www.elastic.co/guide/en/elasticsearch/reference/2.4/indices-create-index.html 创建索引1curl -XPUT &quot;http://192.168.1.99:9200/test&quot; //创建test的索引 1234&#123; &quot;ok&quot;: true, &quot;acknowledged&quot;: true&#125; 删除索引1curl -XDELETE &quot;http://192.168.1.99:9200/test&quot; // 删除索引 1234&#123; &quot;ok&quot;: true, &quot;acknowledged&quot;: true&#125; 以下开始使用Kibana的Sense 来简化curl的操作 查看索引1GET /test 创建索引并设置Type和Mapping 快捷键 Cmd-‘ 引用 Cmd-B 加粗 Cmd-E 清除Block Cmd-H 标题Header变小 Cmd-I 斜体 Cmd-K 链接 Cmd-L 无序列表 Cmd-P Preview Cmd-Alt-C 代码块 Cmd-Alt-I 插入图片 Cmd-Alt-L 有序列表 Shift-Cmd-H 标题Header变大 F9 窗口拆分 F11 全屏","categories":[],"tags":[]},{"title":"","slug":"Elastic-Technologies/Elasticsearch-2.4-Restful-Api","date":"2018-12-11T09:52:56.686Z","updated":"2018-12-11T09:52:56.687Z","comments":true,"path":"Elastic-Technologies/Elasticsearch-2.4-Restful-Api/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Elasticsearch-2.4-Restful-Api/","excerpt":"","text":"title: 文档的title，页面及列表都会展示description: 描述这个页面的信息showOnHome: false… 这个是一个模板, 请务必将showOnHome 修改为true 欢迎使用本Markdown编辑器使用simplemde-plus，用它写博客，将会带来全新的体验哦： Markdown和扩展Markdown简洁的语法 代码块高亮 图片链接和图片上传 丰富的快捷键 快捷键 Cmd-‘ 引用 Cmd-B 加粗 Cmd-E 清除Block Cmd-H 标题Header变小 Cmd-I 斜体 Cmd-K 链接 Cmd-L 无序列表 Cmd-P Preview Cmd-Alt-C 代码块 Cmd-Alt-I 插入图片 Cmd-Alt-L 有序列表 Shift-Cmd-H 标题Header变大 F9 窗口拆分 F11 全屏","categories":[],"tags":[]},{"title":"Elasticsearch 2.X 自定义字段的Mapping","slug":"Elastic-Technologies/Elasticsearch-2-X-Mapping","date":"2018-12-11T09:52:56.686Z","updated":"2018-12-11T09:52:56.686Z","comments":true,"path":"Elastic-Technologies/Elasticsearch-2-X-Mapping/","link":"","permalink":"https://choelea.github.io/Elastic-Technologies/Elasticsearch-2-X-Mapping/","excerpt":"","text":"说到Mapping大家可能觉得有些不解，其实我大体上可以将Elasticsearch理解为一个RDBMS（关系型数据库，比如MySQL），那么index 就相当于数据库实例，type可以理解为表,这样mapping可以理解为表的结构和相关设置的信息（当然mapping有更大范围的意思）。 默认情况不需要显式的定义mapping， 当新的type或者field引入时，Elasticsearch会自动创建并且注册有合理的默认值的mapping(毫无性能压力)， 只有要覆盖默认值时才必须要提供mapping定义。 引用博客：http://blog.csdn.net/top_code/article/details/50767138 术语term - individual word （拆分后的最小单词） Mapping 简介Elasticsearch Reference [2.4] » MappingMapping是用来定义文档及包含字段的保存和索引的方式。 Why接触mapping是因为要收集除了log之外的业务信息。 业务log和系统log不同，很多的自定义字段，并将这些信息推送到单独的index。 最终目的是用过kibana的图形化的展示来统计和分析。当我们要统计比如：用户的访问排名（字段名：user：test@gmail.com）。 当没有设置任何mapping的时候，ES会采用动态mapping（Dynamic Mapping），针对String的字段默认的index方式是：analyzed。这种方式下，test@gmail.com 会被拆分成test和gmail.com(怎么拆分取决于用什么analyzer)。这样不便于统计，这里我们必须显示地去设置mapping。 Mapping parameters » index 通过kibana去选择analyzed的字段去做terms aggregation可以看到对应的warning信息 自定义mapping可以通过API 去自定义mapping。 （这个最好在数据开始index之前，因为数据index的时候会动态设置mapping，再去修改会出现一些冲突）新增加的字段可以继续通过修改mapping来增加。 ES 支持一个index多个type，mapping可以针对单个type也可以针对index。示例： 12345678910curl -XPUT http://localhost:9200/business-index-*/_mapping/biz -d &apos;&#123; &quot;properties&quot; : &#123; &quot;uri&quot; : &#123;&quot;type&quot;: &quot;string&quot;,&quot;index&quot; : &quot;not_analyzed&quot;&#125;, &quot;user&quot; : &#123;&quot;type&quot;: &quot;string&quot;, &quot;index&quot; : &quot;not_analyzed&quot;&#125;, &quot;keyword&quot; : &#123;&quot;type&quot;: &quot;string&quot;, &quot;index&quot; : &quot;not_analyzed&quot;&#125;, &quot;responseStatus&quot; : &#123; &quot;type&quot; : &quot;integer&quot; &#125;, &quot;responseTime&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125;&#125;&apos;; 自定义template对于确定的index，通过mapping的方式就可以达到我们的目的。 比如： 商品的索引，这个index不会变，里面的数据document会增删改查，但是index始终在那里。但是对于类似log和数据分析的数据，这些数据会惊人的速度增加，如果放在一个index就不现实。 所以ELK就有了 “time-based index pattern“ , 通过这种方式可以每天或者每月生成一个index文件。比如logstash的日志： logstash-2016.08.20 针对这种场景，就需要引入更高一层的配置: Index Template设定自己的template的示例如下： 1234567891011121314151617181920212223242526272829303132curl -XPUT http://localhost:9200/_template/business -d &apos;&#123; &quot;template&quot;: &quot;business*&quot;, &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;properties&quot;: &#123; &quot;uri&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125;, &quot;user&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125;, &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125;, &quot;responseStatus&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;responseTime&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125;&#125;&apos;; The settings and mappings will be applied to any index name that matches the business* template","categories":[],"tags":[]},{"title":"Windows 常用命令","slug":"Dev-Ops/Windows-Commands","date":"2018-12-11T09:52:56.672Z","updated":"2018-12-11T09:52:56.672Z","comments":true,"path":"Dev-Ops/Windows-Commands/","link":"","permalink":"https://choelea.github.io/Dev-Ops/Windows-Commands/","excerpt":"","text":"设置环境变量doc 窗口设置环境变量1set MAVEN_OPTS=-Xmx1024m -XX:MaxPermSize=512m 删除服务删除服务名为mysql的服务： sc delete mysql 端口相关端口占用的应用的PID1netstat -aon|findstr &quot;8599&quot; 结果如下： （PID为2948）12C:\\Documents and Settings\\XPMUser&gt;netstat -aon|findstr &quot;8599&quot; TCP 0.0.0.0:8599 0.0.0.0:0 LISTENING 2948 对应PID的进程1tasklist|findstr &quot;2948&quot; 结果如下12C:\\Documents and Settings\\XPMUser&gt;tasklist|findstr &quot;2948&quot;tomcat6.exe 2948 RDP-Tcp#4 0 44,072 K 或者：打开任务管理器，切换到进程选项卡，在PID一列查看2720对应的进程是谁，如果看不到PID这一列，点击查看—&gt;选择列，将PID(进程标示符)前面的勾打上，点击确定。 结束进程结束该进程：在任务管理器中选中该进程点击”结束进程“按钮，或者是在cmd的命令窗口中输入：taskkill /f /t /im Tencentdl.exe。 列出文件夹下面的文件名称创建一个bat文件, 加入下面的内容。 将这个bat文件放入文件夹内运行即可。1DIR *.* /B&gt; LIST.TXT","categories":[],"tags":[]},{"title":"Shell 脚本学习笔记","slug":"Dev-Ops/Shell-learning-notes","date":"2018-12-11T09:52:56.671Z","updated":"2018-12-11T09:52:56.671Z","comments":true,"path":"Dev-Ops/Shell-learning-notes/","link":"","permalink":"https://choelea.github.io/Dev-Ops/Shell-learning-notes/","excerpt":"","text":"括号的使用说明参考：Double parenthesis with and without dollar $(…) means execute the command in the parens and return its stdout. 12$ echo &quot;The current date is $(date)&quot;The current date is Mon Jul 6 14:27:59 PDT 2015 (…) means run the commands listed in the parens in a subshell. Example: 123$ a=1; (a=2; echo &quot;inside: a=$a&quot;); echo &quot;outside: a=$a&quot;inside: a=2outside: a=1 $((…)) means perform arithmetic and return the result of the calculation. Example: 12$ a=$((2+3)); echo &quot;a=$a&quot;a=5 ((…)) means perform arithmetic, possibly changing the values of shell variables, but don’t return its result. Example: 12$ ((a=2+3)); echo &quot;a=$a&quot;a=5 ${…} means return the value of the shell variable named in the braces. Example: 12$ echo $&#123;SHELL&#125;/bin/bash {…} means execute the commands in the braces as a group. Example: 12$ false || &#123; echo &quot;We failed&quot;; exit 1; &#125;We failed 有用脚本收集文件读取读取文件目录的所有文件，按行读取每个文件，判断行文字是否包含特定字符串；如果包含，通过特殊字符来split并输出想要的值。 1234567891011#!/bin/shfor filename in /home/okchem/mysqlbackup/*.sql; do while IFS= read line do # display $line or do somthing with $line if [[ $line == *&quot;/ocf/&quot;* ]]; then SUBSTRING=$(echo $line| cut -d&apos;`&apos; -f 2) # 用&apos;`&apos;来拆分,输出数组第二个 echo $SUBSTRING fi done &lt;&quot;$&#123;filename&#125;.sql&quot;done","categories":[],"tags":[]},{"title":"Nginx 问题收集","slug":"Dev-Ops/Nginx-Technologies","date":"2018-12-11T09:52:56.670Z","updated":"2018-12-11T09:52:56.671Z","comments":true,"path":"Dev-Ops/Nginx-Technologies/","link":"","permalink":"https://choelea.github.io/Dev-Ops/Nginx-Technologies/","excerpt":"","text":"收集在使用Nginx过程中遇见的问题。 知识积累负载均衡平均负载示例如下; 以下配置必须保证两个实例都正常运行在，因为这个配置并不会failover。1234567891011121314151617upstream backend &#123; server 127.0.0.1:8080; server 127.0.0.1:8081;&#125;server &#123; listen 80; server_name auth.jiu-shu.com; location / &#123; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://backend; client_max_body_size 10m; &#125; &#125; 上例中的8080 和 8081 端口都是Spring Boot的app。由于Java 是多线程的程序，在同一个虚拟机上运行多个实例并非最佳实践；这里只是方便测试。 问题收集反向代理后request的host和schema和浏览器请求不一致反向代理后下面如果不加proxy_set_header的两行，那么在microservice这个服务中，request.getScheme() + &quot;://&quot; + request.getServerName() 就会变成http://microservice.dev.com, nginx rewrite 之后，就可以获取到：http://www.dev.com123456server_name www.dev.com;location / &#123; proxy_set_header Host $host; proxy_set_header X-Scheme $scheme; proxy_pass http://microservice.dev.com:8091; &#125; bind() to 0.0.0.0:80 failed (98: Address already in use)启动碰见以上问题，有两种可能 先检查80端口是否已经被其他http server占用 sudo netstat -nlpt remove the IPv6 bind block (something along the lines of ::1:80。 参考：http://serverfault.com/questions/520535/nginx-is-still-on-port-80-bind-to-0-0-0-080-failed-98-address-already-in 403 forbidden (13: Permission denied)参考：Nginx报错403 forbidden (13: Permission denied)的解决办法解决办法一： 关闭 SELinux （在了解了SELinux的重要性后，决定继续寻找更好的解决办法） 需要进一步了解SELinux相关，需要解决办法二：（感谢Zeal老师给出的解决方案） Every directory has a SeLinux context and the default ‘Document Root’ ( /var/www/html ) has an context which allows the nginx / apache user to access the directory.The new ROOT ( /data/images ) will not have the same context and thus SeLinux is blocking the access.You can verify with ls -lZ /Default-Document-Root and verify the context and associate the same context to /data/images.This should ideally solve the issue, can you try and verify once :-chcon -R -u system_u -t httpd_sys_content_t /data/ 相信ftp等服务，如果更改了根目录，也会有同样的问题。需要更深入的对SELinux学习。","categories":[],"tags":[]},{"title":"Maven父子工程的搭建","slug":"Dev-Ops/Maven-usage-of-parent","date":"2018-12-11T09:52:56.669Z","updated":"2018-12-11T09:52:56.670Z","comments":true,"path":"Dev-Ops/Maven-usage-of-parent/","link":"","permalink":"https://choelea.github.io/Dev-Ops/Maven-usage-of-parent/","excerpt":"","text":"尝试dubbo+spring的同时，总结下通过maven创建父子工程的方法。（不考虑unit test） 版本Spring Boot： 1.4.7.RELEASEMaven： 3.2.5 工具eclipse 参考https://github.com/dubbo/dubbo-spring-boot-projecthttp://blog.csdn.net/yaerfeng/article/details/26448417http://blog.csdn.net/isea533/article/details/73744497 maven 国内镜像如果不翻墙，下载maven的依赖相当慢，可以添加阿里云的镜像， 速度相当快。修改conf文件夹下的settings.xml文件，添加如下镜像配置：12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; 步骤创建父maven工程创建普通的maven工程，参考如下截图 填写参数 删除无用文件夹 修改pom.xml packaging 从jar改成pom &lt;packaging&gt;pom&lt;/packaging&gt; 添加spring-boot-starter-parent，添加dependency management。（maven的配置解释参考：http://www.blogjava.net/hellxoul/archive/2013/05/16/399345.html）修改后配置如下： 123456789101112131415161718192021222324252627282930313233343536&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.dubboot&lt;/groupId&gt; &lt;artifactId&gt;dubboot-example&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;dubboot-example&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.7.RELEASE&lt;/version&gt; &lt;!-- keep the version same with $&#123;springboot.version&#125; --&gt; &lt;/parent&gt; &lt;dependencyManagement&gt; &lt;!-- 存在的价值只是为了方便管理版本 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.dubbo.springboot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-dubbo&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 方式一：创建子maven子工程 （dubbo 服务接口） 选中父maven工程右键，新建maven module，输入相关参数即可。 - 工程导入后删除测试相关：pom.xml 的junit依赖及测试相关java文件夹。 pom.xml 添加 &lt;packaging&gt;jar&lt;/packaging&gt;方式二：创建子maven子工程 （Spring Boot， dubbo 服务实现）从https://start.spring.io/ 创建, 添加依赖，JPA， Validation, Mysql 及其他依赖项（不选Spring Cloud 相关）。下载后解压至父maven工程，修改pom.xml 中的parent使其匹配父工程。 12345&lt;parent&gt; &lt;groupId&gt;com.dubboot&lt;/groupId&gt; &lt;artifactId&gt;dubboot-example&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt; 在父工程中添加module： 123&lt;modules&gt; &lt;module&gt;dubboot-jpa&lt;/module&gt;&lt;/modules&gt; 然后可以顺利将子工程导入eclipse。","categories":[],"tags":[]},{"title":"利用Nexus搭建Maven私服","slug":"Dev-Ops/Maven-Private-Repository-Server","date":"2018-12-11T09:52:56.668Z","updated":"2018-12-11T09:52:56.669Z","comments":true,"path":"Dev-Ops/Maven-Private-Repository-Server/","link":"","permalink":"https://choelea.github.io/Dev-Ops/Maven-Private-Repository-Server/","excerpt":"","text":"阐述如果利用Nexus来快速搭建maven仓库的私有服务器私服搭建Docker Hub链接地址： https://hub.docker.com/r/sonatype/nexus/ docker pull sonatype/nexusmkdir /data/nexus-data &amp;&amp; chown -R 200 /data/nexus-datadocker run -d -p 8081:8081 –name nexus -v /data/nexus-data:/nexus-data sonatype/nexus3​ 本地Maven配置​修改Maven的全局setting.xml文件如下： 文件路径： $MAVEN_HOME/conf/setting.xml mirrors节点加入如下内容 nexus * Nexus http://192.168.1.80:8081/repository/maven-public/ profiles节点加入如下内容 nexus central http://central true true central http://central true true activeProfiles​节点加入 nexus​ 对于Snapshot的jar，如果想及时的更新，可以在maven参数中加上-U，就可以获得最新的jar包。本地组件deploy除了配置本地Maven配置外，还需要在setting.xml文件中加入如下内容： servers节点 maven-releases admin admin123 maven-snapshots admin admin123 项目的pom.xml文件，加入如下配置： maven-releases http://192.168.1.​80:8081/repository/maven-releases/ maven-snapshots http://192.168.1.80:8081/repository/maven-snapshots/ ​","categories":[],"tags":[]},{"title":"Java 开发工具使用技巧收集","slug":"Dev-Ops/Java-IDE-Useful-Skills","date":"2018-12-11T09:52:56.667Z","updated":"2018-12-11T09:52:56.668Z","comments":true,"path":"Dev-Ops/Java-IDE-Useful-Skills/","link":"","permalink":"https://choelea.github.io/Dev-Ops/Java-IDE-Useful-Skills/","excerpt":"","text":"EclipseEclipse 提速https://blog.csdn.net/leolu007/article/details/53541641 Remote System Explorer Operation卡死第一步：Eclipse -&gt; Preferences -&gt; General -&gt; Startup and Shutdown.不要勾选 RSE UI.第二步：Eclipse -&gt; Preferences -&gt; Remote Systems. 取消勾选 Re-open Remote Systems view to previous state. 在Eclispe中使用Git 命令参考： https://blog.csdn.net/wu_cai_/article/details/71637199 （建议选择git-bash）直接在eclipse中使用并不是很方便，但是可以快速打开git-bash. 配置方式参考上面。 调试断点出用于打印信息的代码收集Request的Header信息12345java.util.Enumeration headerNames = req.getHeaderNames();while(headerNames.hasMoreElements()) &#123; String headerName = (String)headerNames.nextElement(); System.out.println(&quot;Header Name - &quot; + headerName + &quot;, Value - &quot; + req.getHeader(headerName));&#125;","categories":[],"tags":[]},{"title":"Git 常用命令","slug":"Dev-Ops/Git-Commands","date":"2018-12-11T09:52:56.666Z","updated":"2018-12-11T09:52:56.667Z","comments":true,"path":"Dev-Ops/Git-Commands/","link":"","permalink":"https://choelea.github.io/Dev-Ops/Git-Commands/","excerpt":"","text":"帮助命令1git help command // eg: git commit help windows 打开默认的浏览器显示帮助内容， mac直接显示 配置1234git config --global setting value示例：git config --global user.name &quot;Your Name&quot;示例：git config --global user.email &quot;you@someplace.com&quot;git config --global --list // 列出全局配置项 配置内容保存在当前用户目录下的.gitconfig文件中 本地命令设置邮箱设置全局邮箱方式一： 运行命令： git config --global user.email &quot;joe.lea@foxmail.com&quot;方式二: 编辑文件 .gitconfig, 一般在用户目录下，上面的命令运行后也同样会修改这个文件123[user]name =xiaomingemail = xiaoming@qq.com 设置项目的提交邮箱编辑文件 .git/config 即可 github 项目只有设置了提交者的邮箱，才会在contibutors中you展示。 比如：https://github.com/choelea/markdown-cms/graphs/contributors 初始化方式一：12cd projects/git init git-demo // projects下面创建文件夹 git-demo, 并初始化； 初始化其实就是在文件夹下面创建了相关内容存放在.git 隐藏文件夹下面 方式二：1234cd projects/mkdir websitecd website/git init // 初始化 方式三：大多数的方式，我们从clone一个git 库开始的。1git clone &apos;url&apos; 查看本地分支12git branch -vv // 列出本地分支 * 标识当前分支git branch -a // 列出所有分支 删除本地分支1234git branch -d &lt;BranchName&gt;``` ### 查看状态 git status // Shows which files have been modified in the working directory vs Git’s staging area.12### 添加新文件 git add file-name // Adds the new or newly modified file-name to Git’s staging area (index).1234&gt; 当很多文件修改，而且这些文件不属于同一个功能修改，想分开多个commit来提交的时候，可选择通过`git add &lt;file&gt;` 先将指定的文件Stage，然后使用`git commit -m ` 来只提交stage的文件。### Commit 修改 git commit -m “A really good commit message” // Commits all files currently in Git’s staging area.123&gt; 上面的命令只有所有的文件都在staging area在有效。 `git commit -am &quot;A really good commit message&quot;` 可以省掉git add这步，不过新文件必须先add下。### 回滚 git add . // Add all new and newly modified files.git reset HEAD file-name // Unstage the specified file from stage area. 修改的内容还在git checkout – file-name // 回滚本次修改git reset –hard HEAD^ // 回滚到远程仓库的版本，放弃本地所有包括commit的修改1### 检查修改内容 git diff // 查看unstage状态下的文件的修改内容，staged的无法查看1### 合并到上次提交 git add . // 将修改的文件 stagegit commit –amend // 将当前的staged的修改合并到上次commit，并打开编辑器修改commitgit commit –amend -m “New commit message” // 将当前的staged的修改合并到上次commit，并实用新的Message123&gt; 使用Interactive Rebasing/squash也可以达到合并的效果，区别就是一个是事先（commit 前）就合并，一个是事后（commit 后）合并。### 放弃本地修改或新增的文件放弃modified的文件 git checkout // 重新checkout文件file，相当于丢掉了本地的修改git checkout src/ // 使用通配符来checkout src文件夹下面所有的修改git reset –hard // 丢掉所有的修改modified 文件git clean -fd // 移除所有untrack的文件和文件夹git clean -fd src/ // 移除src目录下面所有的新增的文件123&gt; git clean -fd 中 -f means force, -d means &apos;remove directories&apos;### 切换分支&gt; 切换分支前必须保证工作空间是干净的。（没有未提交的修改和新增） git checkout branchename // 切换到branchname分支12### 提交历史 log日志 git log // 默认不用任何参数的话，git log 会按提交时间列出所有的更新，最近的更新排在最上面。git log –oneline –graph –decorate –colorgit log – // 查看某个文件的日志1#### 查看历史提交的内容差异 git log -p -21234 我们常用 -p 选项展开显示每次提交的内容差异，用 -2 则仅显示最近的两次更新：### 移除文件方式一：完全通过git命令 git rm debug.log // remove and stage the changegit commit -m ‘remove file debug.log’12方式二：非git 命令删除文件后，运行下面的命令 git add -u // git 2.0 以前的版本 stage删除的changegit add file-name // git 2.0 后也可以通过这个命令达到上面的命令的效果git commit -m ‘commit message’1### 移动文件 git mv index.html web/ // 移动index.html 到web文件夹内。 命令完成后直接进入staging 状态git commit -m ‘move index.html into web folder’123456### ignore 文件编辑 .gitignore 文件## SSH 命令windows cmd并没有自带ssh命令，我们可以通过git bash命令窗来运行这些命令。假定在当前用户的目录下： cd .sshssh-keygen -t rsa -C “your email” // 生成SSH Key， 将id_rsa.pub公钥配置到github/bitbucket 等服务器上ssh -T git@github.com // 验证SSH 配置成功123## Git Remote 相关命令关联一个远程的Repo。 （针对前两种初始化方式，一般情况用不上） git remote add remote-name remote-repository-location // 示例: git remote add origin git@github.com:choelea/keycloak-demo.gitgit push -u remote-name branch-name // 示例: git push -u origin master; The -u parameter is needed the first time you push a branch to the remote.git remote -v // list the names of all the remote repositories12关联远程repo之前需要先在git服务器上创建对应的repo，如果采用的是github，在创建repository后，会有如下的提示：![Git-Push-Remote](http://tech.jiu-shu.com/Dev-Ops/git-push-remove.png) git pull origin master // 下载当前分支远程修改；每次push前都应该先pull123456## Git Rebase关于rebase和merge的区别，建议参考：[Merging vs Rebasing](https://www.atlassian.com/git/tutorials/merging-vs-rebasing)&gt; 一定要看看 &apos;The Golden Rule of Rebasing&apos;这部分.rebase 和 merge都是应该发生在分支之间的事情，当然在同一个分支上有时也需要。（可能不是最佳实践，git的开发流程一般建议创建单独的feature分支来完成不同的story，避免出现多人在同一个分支上直接commit）直接在当前分支做rebase git fetch // 这一步必须git rebase // 将未push的commits 放至remote所有commits之上// 修复冲突 如果有冲突必须进行修复，完成后，注意提示。一般需要git add 命令来stage下 ，接着git rebase –continue 至到没有任何冲突123## Squash通过Interactive Rebasing来完成当前分支的commits的squash git rebase -i // 列出所有未push的commit，注意是倒序1234567根据提示编辑来达到squash的作用。![git-rebase](http://tech.jiu-shu.com/Dev-Ops/git-rebase.png)将第二个commit（435d22b）修改为:`pick 435d22b ...` 即将这个commit压缩至上面的commit，并放弃当前的commit message。&gt; 有些公司会很强调squash。 git估计本地多次提交防止丢失，所以git的commit有可能会很多；而svn的commit就意味着修改可以被其他用户拉取到， 所以svn的每一次commit都要保证系统可以运行，svn的commit会偏少。svn的代码更新时间取决于文件多少和大小；git的代码拉取时间取决于commit的多少。所以。。。是每次提交尽量合理依然很重要，squash/Ineractive Rebasing 很实用。## Changing remote URLrepo换了名字，或者之前是https clone下来的，现在想换成ssh；这些情况都面临着修改远程的URL。 git remote -v // 查看当前的地址git remote set-url origin git@github.com:choelea/tech-docs.git` https的URL一般来说push代码是需要用户明和密码；而ssh的不需要。","categories":[],"tags":[]},{"title":"Centos 常用命令","slug":"Dev-Ops/Centos-Common-Commands","date":"2018-12-11T09:52:56.666Z","updated":"2018-12-11T09:52:56.666Z","comments":true,"path":"Dev-Ops/Centos-Common-Commands/","link":"","permalink":"https://choelea.github.io/Dev-Ops/Centos-Common-Commands/","excerpt":"","text":"以下命令仅在centos7上验证过 软件安装yum 安装示例如下：1sudo yum install elasticsearch rpm 包安装1sudo rpm -ivh kibana-4.6.6-x86_64.rpm // 安装后通过 sudo service kibana start 来启动 查看安装程序路径1sudo rpm -ql kibana // 查看到安装在了/opt/kibana 开放端口12$ sudo firewall-cmd --zone=public --add-port=80/tcp --permanent$ sudo firewall-cmd --reload netstat 使用查看某个服务是否在运行1sudo netstat -aple | grep nginx 只查看tcp或者udp的connections需要添加-t参数: sudo netstat -nplt 更多更实用的netstat命令参考：Linux netstat 命令示例 查看centos 版本cat /etc/centos-release设置环境变量 12export KAFKA_HOME=/home/osboxes/kafka_2.10-0.10.0.1echo $KAFKA_HOME 磁盘空间1df -h 1free 查看目录所占空间大小1du -smh * 统计文件夹下面的文件数量1ls -1 | wc -l grep 搜索文件内容指定的文件类型中查找当前子目录中查找： grep -r abcd *.properties 当前子目录递归查找含有abcd 的*.properties 文件指定目录及子目录中查找：grep -r 3306 /home/okchem/storage92g/srm/ 拷贝整个文件夹1cp -avr /home/vivek/letters /usb/backup 用户和组 /etc/group file that lists all users groups 可以使用cut命令列出来cut -d: -f1 /etc/group 查看当前用户的group: $ groups查看用户的group: $ groups root id -Gn root添加用户到组: $ sudo usermod -a -G osboxes nginx 添加用户nginx到组osboxes usermod -a -G &lt;groupname&gt; username 添加完成请用groups &lt;username&gt; 来验证获取组的所有用户: getent group kibana 其他有用资源： CentOS7之新建用户与SSH登陆 文件权限修改文件[夹]ownerchown 代表change owner；chown --help 提供了更详细的信息1sudo chown -R okchem:root /ebs 修改文件[夹]访问权限chmod 代表change mode;例如：chmod 644 important.txt owner可读可写,group可读，others可读 First position refers to the user. Second refers to the group of the user, and the third refers to all others.4 = read 2 = write 1 = execute 文件权限更详细的解释可以参考：Linux File Permissions PS 命令查看java进程 ps -ef|grep java产看进程的详细信息 ps -auxwe | grep subscribe命令行快捷键CTRL-a 光标移至行首CTRL-e 光标移至行尾CTRL-u 删除整行CTRL-h 删除光标前字符 gzip / gunzip 压缩单个文件 gzip fileName 压缩后的名字=原文件名字加上后缀.gz 解压缩单个文件gunzip filename 或者 gzip -d filenamegzip 不能用来压缩整个文件夹至一个.gz 文件。压缩整个文件夹请参考targ + gzip 命令 即： tar -z命令。 gzip -r dictName 命令会压缩整个文件夹dictName 里面的所有文件，每个文件被压缩成一个单独的*.gz 文件 tar 命令gzip / bzip2 是用来压缩单个文件， tar是用来归档。 所以tar结合gzip/bzip2 可以方便的进行整个文件夹的压缩及归档。压缩整个文件夹tar -zcvf outputFileName folderToCompressExamples bzip2 sftp 命令sftp登录1sftp name@123.21.331.1 sftp 下载文件夹1get -r folder /home/joe/ sftp 上传文件1put /name1.html /name2/ crontab 命令创建执行任务， 添加cron job参考cronjob crontab -l编辑cronjob crontab -e 120 1 * * * /data/scripts/mysql-job.sh A20 1 * * 0 /data/scripts/mysql-job.sh I 两个cron job 分别： 每天1点执行 每周日1点20 执行 参考：crontab 时间可以参考： https://www.cnblogs.com/intval/p/5763929.html","categories":[],"tags":[]},{"title":"Mysql 运维相关脚本收集","slug":"Database-Technologies/Mysql-Administration","date":"2018-12-11T09:52:56.662Z","updated":"2018-12-11T14:14:40.485Z","comments":true,"path":"Database-Technologies/Mysql-Administration/","link":"","permalink":"https://choelea.github.io/Database-Technologies/Mysql-Administration/","excerpt":"","text":"mysql 版本： 5.6 建库及用户创建数据库dbname及用户dbuser/dbpassword 并授权数据库全不权限给用户dbuser12CREATE DATABASE IF NOT EXISTS `dbname` /*!40100 DEFAULT CHARACTER SET utf8 COLLATE utf8_bin */grant all privileges on dbname.* to dbuser@localhost identified by 'dbpassword'; SQL 收集找出有记录的表1SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = &apos;okchem&apos; and table_rows &gt; 0; 快速删除树形表数据如何快速删除树形比如：ProductCategory 这类模型的数据：123SET FOREIGN_KEY_CHECKS=0;DELETE FROM okchem.ProductCategory where id &gt; 0; -- id&gt;0 可以去除错误SET FOREIGN_KEY_CHECKS=1; 采用where条件where id &gt; 0可以去除如下错误：Error Code: 1175. You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column To disable safe mode, toggle the option in Preferences -&gt; SQL Editor and reconnect. 快速查询表的依赖查询表依赖那些表和查询那些表依赖此表； 查询我依赖的：123456789SELECT TABLE_NAME, COLUMN_NAME, CONSTRAINT_NAME, REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAMEFROM INFORMATION_SCHEMA.KEY_COLUMN_USAGEWHERE TABLE_SCHEMA = &quot;schemaName&quot; AND TABLE_NAME = &quot;TableName&quot; AND REFERENCED_COLUMN_NAME IS NOT NULL; 查询依赖‘我的’：123456789 SELECT TABLE_NAME, COLUMN_NAME, CONSTRAINT_NAME, REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAMEFROM INFORMATION_SCHEMA.KEY_COLUMN_USAGEWHERE TABLE_SCHEMA = &quot;schemaName&quot; AND REFERENCED_TABLE_NAME = &quot;TableName&quot;; 删除重复的行12345DELETE t1 FROM contacts t1 INNER JOIN contacts t2 WHERE t1.id &lt; t2.id AND t1.email = t2.email; // 当表的记录太多，这种join很危险， 最好的方式是先查出来重复的email，再加上in的条件12345DELETE t1 FROM contacts t1 INNER JOIN contacts t2 WHERE t1.id &lt; t2.id AND t1.email = t2.email and t2.email in (&apos;..&apos;,,,,,,,); 使用函数为空的时候给默认值1select ifnull(p.isActive,0) from product 转换成JSON创建FunctionSplit delimited strings参考： https://blog.fedecarg.com/2009/02/22/mysql-split-string-function/ 123456789CREATE FUNCTION SPLIT_STR( x VARCHAR(255), delim VARCHAR(12), pos INT)RETURNS VARCHAR(255)RETURN REPLACE(SUBSTRING(SUBSTRING_INDEX(x, delim, pos), LENGTH(SUBSTRING_INDEX(x, delim, pos -1)) + 1), delim, &apos;&apos;); Mysql 分库备份脚本1234567891011121314151617#!/bin/sh#Backup databases into separated files excluding system schemasBACKUP_FOLDER=/home/okchem/mysqlbackupMYUSER=userMYPASS=passwordSOCKET=/data/mysql/mysql.sockMYCMD=&quot;mysql -u$MYUSER -p$MYPASS -S $SOCKET&quot;MYDUMP=&quot;mysqldump -u$MYUSER -p$MYPASS -S $SOCKET&quot;mkdir -p $&#123;BACKUP_FOLDER&#125;#for database in `$MYDUMP -e &quot;show databases;&quot;|sed &apos;1,2d&apos;|egrep -v &quot;mysql|schema&quot;`for database in `$MYCMD -e &quot;show databases;&quot; | egrep -Evi &quot;database|mysql|schema|test&quot;`do $MYDUMP $database &gt;$&#123;BACKUP_FOLDER&#125;/$&#123;database&#125;_$(date +%Y%m%d).sql #If compression is needed, use this command: $MYDUMP $database |gzip &gt;/server/backup/$&#123;database&#125;_$(date +$F).sql.gzdone Mysql 客户端导出数据在mysql 服务端可以很方便的导出到文件，也有灵活的选择。 如果需要导出的文件到其他服务器，不在mysql服务器上。 有两个选择： 在mysql 服务器上导出文件，通过sftp上传至目标机器 在目标机器安装mysql 客户端，通过shell 脚本来导出数据 （此篇关注点） 验证环境Linux 系统：Centos 7 安装Mysql Client参考：Installing MySQL on Linux Using RPM Packages from Oracle Shell 脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/sh############################################################################################################################################### This script is used to retrieve data from mysql and output it into txt file. Also it will generate md5 file which can be used to verify the integrity.# Script will make folder named \"YYYYMMDD\", also the file name will follow the pattern A/I&#123;tableName&#125;YYYYMMDD&#123;6 sequence number&#125; such as I0100320170303000001############################################################################################################################################################Global Configuration begins ##################### Root folder where the data will be storedBEE_ROOT_GLOBAL=/data/b2bbuyerdataMYSQL_HOST=192.168.1.90MYSQL_PORT=3306MYSQL_USERNAME=usernameMYSQL_PASSWD=password##############Global Configuration ends ##################### exportAndMD5Sum querySql tableName. Output the query result into tableNameYYYYMMDD000001.txt and tableNameYYYYMMDD000001.md5# .md5 file is used to verify data integrity. exportAndMD5Sum()&#123; if [ \"$#\" != 2 ];then echo \"Usage: exportAndMD5Sum querySql tableName\"; exit; fi # Starting export data using mysql command SQL=$1; tableName=$2; TIMESTAMP=`date +%Y%m%d` BEE_ROOT=$&#123;BEE_ROOT_GLOBAL&#125;/$&#123;TIMESTAMP&#125; _tmpFile=$&#123;BEE_ROOT&#125;/$&#123;tableName&#125;$&#123;TIMESTAMP&#125;000001.tmp; destFile=$&#123;BEE_ROOT&#125;/$&#123;tableName&#125;$&#123;TIMESTAMP&#125;000001.AVL; destMD5File=$&#123;BEE_ROOT&#125;/$&#123;tableName&#125;$&#123;TIMESTAMP&#125;000001.CHK; # Create Folder [ ! -d \"$BEE_ROOT\" ] &amp;&amp; mkdir \"$BEE_ROOT\" # Mysql command to output data into file `mysql -h $&#123;MYSQL_HOST&#125; -p$&#123;MYSQL_PORT&#125; -u $&#123;MYSQL_USERNAME&#125; --password=$&#123;MYSQL_PASSWD&#125; -e \"$&#123;SQL&#125;\" &gt; \"$&#123;_tmpFile&#125;\"` # If not empty(has records) change the file name, otherwise remove it. if [ -f \"$_tmpFile\" ] &amp;&amp; [ -s \"$_tmpFile\" ] then mv $&#123;_tmpFile&#125; $&#123;destFile&#125; #`md5sum $&#123;destFile&#125; &gt; $&#123;destMD5File&#125;` md5=($(md5sum $&#123;destFile&#125;)) echo $md5 &gt; $&#123;destMD5File&#125; else rm $&#123;_tmpFile&#125; fi&#125;if [ \"$1\" = \"I\" ]; then echo \"Starting export all data from mysql .............\" exportAndMD5Sum \"SELECT username,country,source,city,email,first_name,last_name,province,status,CAST(is_reveive_email AS UNSIGNED) AS is_reveive_email,created_stamp,last_updated_stamp FROM b2bbuyer.user\" \"I01001\" exportAndMD5Sum \"select u.username,a.address,a.city,a.company_name,a.country,a.first_name,CAST(a.is_default AS UNSIGNED) AS is_default ,a.last_name,a.province,a.tel_country_code,a.tel_ext,a.tel_no,a.zip_code,a.created_stamp,a.last_updated_stamp from b2bbuyer.user u inner join b2bbuyer.user_delivery_address a where a.user_id=u.id\" \"I01002\" exportAndMD5Sum \"select u.username,c.email,c.address,c.city,c.company_name,c.contact,c.country,c.fax_country_code,c.fax_ext,c.fax_tel_no,c.main_products,c.province,c.register_no,c.tax_no,c.tel_country_code,c.tel_ext,c.tel_no,c.website from b2bbuyer.user u inner join b2bbuyer.user_company c where c.user_id=u.id\" \"I01003\"else echo \"Starting export yesterday's data from mysql .............\" exportAndMD5Sum \"SELECT username,country,source,city,email,first_name,last_name,province,status,CAST(is_reveive_email AS UNSIGNED) AS is_reveive_email,created_stamp,last_updated_stamp FROM b2bbuyer.user where last_updated_stamp &lt; (UNIX_TIMESTAMP(CURDATE())*1000) and last_updated_stamp &gt; ((UNIX_TIMESTAMP(CURDATE())-60*60*24)*1000)\" \"A01001\" exportAndMD5Sum \"select u.username,a.address,a.city,a.company_name,a.country,a.first_name,CAST(a.is_default AS UNSIGNED) AS is_default ,a.last_name,a.province,a.tel_country_code,a.tel_ext,a.tel_no,a.zip_code,a.created_stamp,a.last_updated_stamp from b2bbuyer.user u inner join b2bbuyer.user_delivery_address a where a.user_id=u.id and a.last_updated_stamp &lt; (UNIX_TIMESTAMP(CURDATE())*1000) and a.last_updated_stamp &gt; ((UNIX_TIMESTAMP(CURDATE())-60*60*24)*1000)\" \"A01002\" exportAndMD5Sum \"select u.username,c.email,c.address,c.city,c.company_name,c.contact,c.country,c.fax_country_code,c.fax_ext,c.fax_tel_no,c.main_products,c.province,c.register_no,c.tax_no,c.tel_country_code,c.tel_ext,c.tel_no,c.website from b2bbuyer.user u inner join b2bbuyer.user_company c where c.user_id=u.id and c.last_updated_stamp &lt; (UNIX_TIMESTAMP(CURDATE())*1000) and c.last_updated_stamp &gt; ((UNIX_TIMESTAMP(CURDATE())-60*60*24)*1000)\" \"A01003\"fi 添加cron job参考cronjob crontab -l编辑cronjob crontab -e 120 1 * * * /data/scripts/mysql-job.sh A20 1 * * 0 /data/scripts/mysql-job.sh I 两个cron job 分别： 每天1点执行 每周日1点20 执行 参考：crontab 时间可以参考： https://www.cnblogs.com/intval/p/5763929.html Mysql 客户端导入数据从txt文件导入参考： https://blog.csdn.net/huihui520com/article/details/79080512 https://segmentfault.com/a/1190000009333563 123use test;load data infile &apos;D:/tmp/hotwords.txt&apos; into table hot fields terminated by &apos;,&apos; lines terminated by&apos;\\r\\n&apos;;ALTER TABLE okchem.hot ADD `id` INT NOT NULL AUTO_INCREMENT PRIMARY KEY; 需要解决问题：–secure-file-priv option so it cannot execute this statement123windows下：修改my.ini 在[mysqld]内加入secure_file_priv =linux下：修改my.cnf 在[mysqld]内加入secure_file_priv = mysql 数据迁移自增字段问题新增表格，需要将旧的数据迁入新表。Mysql的自增字段默认行为： 取最大的(比如： 创建表后，只插入一条数据， ID直接指定为9， 那么下一条插入的数据在不指定ID值的情况下，ID是10） 删除数据后，ID的起点不会因为删除而改变。 （插入N条数据，假如这N条都是未指定ID的插入，也就是说下一个ID是N+1， 这个时候删除所有的数据，再以不指定ID的方式插入一条数据，这个时候ID是N+1）Mysql 系统变量配置windows 下安装的mysql的配置文件地址从服务列表services.msc 中找到mysql的服务，右键查看属性中的“可执行文件路径”。参考：https://blog.csdn.net/postnull/article/details/72455768Win 7 设置表明区分大小写参考： https://blog.csdn.net/postnull/article/details/72455768在my.ini 文件中添加 lower_case_table_names=2","categories":[],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://choelea.github.io/tags/Mysql/"},{"name":"运维","slug":"运维","permalink":"https://choelea.github.io/tags/运维/"},{"name":"数据库","slug":"数据库","permalink":"https://choelea.github.io/tags/数据库/"}]},{"title":"MongoDB 命令 常用语句","slug":"Database-Technologies/mongodb-command","date":"2018-12-11T09:52:56.662Z","updated":"2018-12-11T14:05:31.644Z","comments":true,"path":"Database-Technologies/mongodb-command/","link":"","permalink":"https://choelea.github.io/Database-Technologies/mongodb-command/","excerpt":"","text":"mongodb 常用命令收集 导出JSON数据1mongoexport -h localhost:27017 -d guide-chem -c product --limit 10000 --skip 10000 --jsonArray -u okchem -p okchem -o /home/okchem/products.json 参数说明 -h 指定host 和端口 -d 指定db -c 指定collection –limit 导出多少条 –skip 跳过多少条 –jsonArray 保存为json数组 -u 指定用户 -p 指定密码 -o 指定导出文件路径output 修改字段名称1db.集合名称.update(&#123;&#125;, &#123;$rename:&#123;&quot;旧键名称&quot;:&quot;新键名称&quot;&#125;&#125;, false, true) 参数说明 第一个false表示：可选，这个参数的意思是，如果不存在update的记录，true为插入新的记录，默认是false，不插入。 第二个true表示：可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。","categories":[],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://choelea.github.io/tags/Mysql/"},{"name":"Mongo","slug":"Mongo","permalink":"https://choelea.github.io/tags/Mongo/"}]},{"title":"Mongodb 和 Mysql 的性能测试","slug":"Database-Technologies/Mongodb-vs-Mysql-basic","date":"2018-12-11T09:52:56.661Z","updated":"2018-12-11T09:52:56.661Z","comments":true,"path":"Database-Technologies/Mongodb-vs-Mysql-basic/","link":"","permalink":"https://choelea.github.io/Database-Technologies/Mongodb-vs-Mysql-basic/","excerpt":"","text":"尝试测试Mongodb 和 Mysql的性能，测试/数据导入代码：github: mongo-vs-mysql 性能比较很复杂，不能简单就说谁的性能高，谁的低。要基于场景，基于并发请求数量来谈，同时也要知道如何调优，本文只是初探，在没有任何调优的基础上，在本地windows 7上进行测试。 版本及环境 操作系统： windows 7 硬件环境： （只做对比，mongodb和mysql都装在同一台机器上） mongodb： 3.2.5 mysql： 5.7 Data-demoData-demo 是一个Spring Boot的项目， 通过Spring Boot的CommandLineRunner来批量动态插入1000,020 条数据。 数据结构采用常用的产品和类目的多对多的设计。 Category 数据如下： id code name ‘1’ ‘cate-1’ ‘Category 1’ ‘2’ ‘cate-2’ ‘Category 2’ ‘3’ ‘cate-3’ ‘Category 3’ ‘4’ ‘cate-4’ ‘Category 4’ Product 数据如下 id code name price ‘1’ ‘p-0’ ‘product 0’ ‘19’ ‘2’ ‘p-1’ ‘product 1’ ‘19’ ‘3’ ‘p-2’ ‘product 2’ ‘19’ … … … … ‘1000000’ ‘p-999999’ ‘product 999999’ ‘19’ ‘1000001’ ‘pp-0’ ‘iphone’ ‘19’ ‘1000002’ ‘pp-1’ ‘iphone’ ‘19’ ‘1000003’ ‘pp-2’ ‘iphone’ ‘19’ ‘1000004’ ‘pp-3’ ‘iphone’ ‘19’ ‘1000005’ ‘pp-4’ ‘iphone’ ‘19’ ‘1000006’ ‘pp-5’ ‘iphone’ ‘19’ ‘1000007’ ‘pp-6’ ‘iphone’ ‘19’ ‘1000008’ ‘pp-7’ ‘iphone’ ‘19’ ‘1000009’ ‘pp-8’ ‘iphone’ ‘19’ ‘1000010’ ‘pp-9’ ‘iphone’ ‘19’ ‘1000011’ ‘pp-10’ ‘iphone’ ‘19’ ‘1000012’ ‘pp-11’ ‘iphone’ ‘19’ ‘1000013’ ‘pp-12’ ‘iphone’ ‘19’ ‘1000014’ ‘pp-13’ ‘iphone’ ‘19’ ‘1000015’ ‘pp-14’ ‘iphone’ ‘19’ ‘1000016’ ‘pp-15’ ‘iphone’ ‘19’ ‘1000017’ ‘pp-16’ ‘iphone’ ‘19’ ‘1000018’ ‘pp-17’ ‘iphone’ ‘19’ ‘1000019’ ‘pp-18’ ‘iphone’ ‘19’ ‘1000020’ ‘pp-19’ ‘iphone’ ‘19’ 最后的二十行是用来方便查询验证的。 Product_Category 中间mapping的表格 Mongo 数据采用了 Nodejs+express+mongoose 来导入mongo的数据. 项目express-mongoose-microservice-api-boilerplate中的config/test.env来配置mongo的数据库地址。npm install 然后运行命令npm run produceTestData 可以初始化1000,020 条产品数据到mongodb。 数据类似mysql的产品数据： 产品 Product1234567891011&#123; &quot;_id&quot;: &quot;59cb4952d44efa2eb45d4bf7&quot;, &quot;code&quot;: &quot;p-0&quot;, &quot;name&quot;: &quot;Product 0&quot;, &quot;price&quot;: 19, &quot;__v&quot;: 0, &quot;categories&quot;: [ &quot;cate-1&quot;, &quot;cate-2&quot; ]&#125; 有20条产品数据的categories中有cate-4 通过查询脚本直接测试：通过Robomongo 连接Mongodb来测试，通过mysql的workbench来完成mysql的脚本查询。 场景一：查询单个类目下的产品mongo 查询所有的cate-4 的产品1db.getCollection(&apos;products&apos;).find(&#123;categories:&apos;cate-4&apos;&#125;) // 初次查询1.321 秒 紧接着的两次查询大概0.791 秒 mysql 查询所有的cate-4 的产品12SELECT * FROM product p inner join product_category pc inner join category c on p.id=pc.product_id and pc.category_id=c.id where c.code ='cate-4'; -- 毫秒级，时间可以忽略不计, 产品和类目的code都是unique的索引，所以查询速度很快SELECT * FROM product p inner join product_category pc inner join category c on p.id=pc.product_id and pc.category_id=c.id where c.name ='Category 4'; -- 6.2秒，name不是索引，所以慢。（索引的用处毫无疑问，无需赘述） 场景二：查询多个类目下的产品查询所有cate-4 加上 cate-5 的产品。（实际上cate-5并不存在，不过不影响测试） mongod1db.getCollection(&apos;products&apos;).find(&#123;categories:&#123;$in:[&apos;cate-4&apos;,&apos;cate-5&apos;]&#125;&#125;) // 0.89 秒； 和查询cate-4的产品相差不多，都是全表扫描 mysql12SELECT * FROM product p inner join product_category pc inner join category c on p.id = pc.product_id and pc.category_id = c.id where c.code = &apos;cate-5&apos; or c.code=&apos;cate-4&apos;; -- 6.177 秒，SELECT * FROM product p inner join product_category pc inner join category c on p.id = pc.product_id and pc.category_id = c.id where c.code in(&apos;cate-5&apos;,&apos;cate-4&apos;); -- 6.24 秒 通过上面的测试可以看出，mysql数据库在数据体量大的时候，用or或者in都有很严重的性能问题，可以考虑使用union来代替。一般电商平台的处理方式：如果是后台维护功能应该从业务上来避免这种场景，如果是前端面向用户的功能，需要引入搜索引擎 比如： elasticsearch 场景三：单表无索引查询名称是iphone的产品 Mysql1select * from product where name=&apos;iphone&apos;; -- 0.546 秒，全表扫描 mongo1db.getCollection(&apos;products&apos;).find(&#123;name:&apos;iphone&apos;&#125;) // 0.428 秒 全表扫描两者并无太大的差距。 场景四：单表索引12db.getCollection(&apos;products&apos;).find(&#123;code:&apos;pp-1&apos;&#125;) select * from product where code=&apos;pp-1&apos;; 一百万条数据，单表索引速度都是毫秒级，时间可以忽略不计。 场景五：单表索引字段使用In来查询场景二我们提到了Mysql中关联表时使用in查询的效率问题。下面测试下单表的In查询效率, 通过测试我们可以发现单表针对索引的in的查询都是毫秒级的。mongodb 历时0.004 sec. 索引被用上了123456789101112131415161718// 非ID的索引字段db.getCollection(&apos;products&apos;).find(&#123; &quot;code&quot;: &#123; &quot;$in&quot;: [ &quot;pp-0&quot;, &quot;pp-1&quot; ] &#125;&#125;)// _id 主键的$in 查询db.getCollection(&apos;products&apos;).find(&#123; &quot;_id&quot;: &#123; &quot;$in&quot;: [ ObjectId(&quot;59cb4952d44efa2eb45d4bf7&quot;), ObjectId(&quot;59cb4952d44efa2eb45d4bf8&quot;) ] &#125;&#125;) Mysql 也是毫秒级，时间忽略不计 1select * from product where code in ('pp-0','pp-4'); // 0.0000 sec","categories":[],"tags":[]}]}